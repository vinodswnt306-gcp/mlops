{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1606927f",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb17777",
   "metadata": {},
   "source": [
    "![title](img/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885643b4",
   "metadata": {},
   "source": [
    "# Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b63ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "+---data\n",
    "|       dataloan.csv\n",
    "|       FE_test_inputs.csv\n",
    "|       validation_data.csv\n",
    "|\n",
    "+---img\n",
    "|       screenshots.png\n",
    "|\n",
    "+---train_pipeline (CI)\n",
    "|       pipeline_components.py\n",
    "|       pipeline_publisher.py\n",
    "|       pipeline_run.py\n",
    "|       cloudbuild.yaml\n",
    "|       Dockerfile\n",
    "|       pipeline_base_image_builder.sh\n",
    "|\n",
    "+---deployment (CD)\n",
    "|       score.py\n",
    "|       requirements.txt\n",
    "|       deployment.yaml\n",
    "|       service.yaml\n",
    "|       cloudbuild.yaml\n",
    "|       Dockerfile\n",
    "|       scoring_image_builder.sh\n",
    "|\n",
    "+---tests\n",
    "|       test_1_FE.py\n",
    "|       test_2_impute_class.py\n",
    "|       test_3_score.py\n",
    "|\n",
    "+---utils\n",
    "|       FE.py\n",
    "|       impute_class.py\n",
    "|\n",
    "|---architecture.pptx\n",
    "|---main.ipynb\n",
    "|---requirements.txt\n",
    "|---secrets.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b85285",
   "metadata": {},
   "source": [
    "# Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95379174",
   "metadata": {},
   "source": [
    "![title](img/index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffbf426",
   "metadata": {},
   "source": [
    "# Step 1 :\n",
    "## Setup a GitHub/CSR/Bitbucket repository containing project folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2333a610",
   "metadata": {},
   "source": [
    "#### 1.1 Details of “train_pipeline” folder"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc219eef",
   "metadata": {},
   "source": [
    "+---train_pipeline\n",
    "|       pipeline_components.py\n",
    "|       pipeline_publisher.py\n",
    "|       pipeline_run.py\n",
    "|       cloudbuild.yaml\n",
    "|       Dockerfile\n",
    "|       pipeline_base_image_builder.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb73c73",
   "metadata": {},
   "source": [
    "##### pipeline_components.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c40da705",
   "metadata": {},
   "source": [
    "Note : There are 2 methods to create component\n",
    "1. Create component from python function and use base_image which can run all steps\n",
    "2. Create component by defining separate docker containers for each step\n",
    "In below code we have used method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e9f7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline/pipeline_components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline/pipeline_components.py\n",
    "# Import all dependencies\n",
    "import subprocess, sys, os\n",
    "from kfp.components import InputPath, InputTextFile, OutputPath, OutputTextFile,OutputArtifact\n",
    "from typing import NamedTuple\n",
    "import kfp.dsl as dsl\n",
    "\n",
    "# Every step in training job requires base image which already contains dependencies\n",
    "# Lets build base image with new tag everytime new code is checked in, as dependencies may change\n",
    "# To have new image name everytime we use git commit id as image tag.\n",
    "\n",
    "# Get git commit id\n",
    "git_commit_id = subprocess.run(['git', 'log','-1', '--pretty=%h'], capture_output=True)\n",
    "\n",
    "# Create container name with new tag\n",
    "os.environ[\"CONTAINER_NAME\"] = 'vinodswnt306/new_public_mlops:' + git_commit_id.stdout.decode('utf-8').replace('\\n','')\n",
    "CONTAINER_NAME = os.environ[\"CONTAINER_NAME\"]\n",
    "\n",
    "# e.g.\n",
    "# CONTAINER_NAME = 'vinodswnt306/new_public_mlops:aada71f'\n",
    "\n",
    "# Step 1 of Kubeflow pipeline \n",
    "# Read the data \n",
    "@dsl.python_component(\n",
    "    name='read_split',\n",
    "    description='',\n",
    "    base_image=CONTAINER_NAME  # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "def read_and_split(gcs_path: str,output_csv: OutputPath(str),mlpipeline_ui_metadata_path: OutputPath('ui')):\n",
    "    \"\"\"\n",
    "    Read and Splits data into train, validation and test set\n",
    "  \n",
    "    Parameters:\n",
    "    gcs_path (str) : Path of input data\n",
    "    output_csv (str) : (internally assigned by kfp) Path where output csv will be stored and \n",
    "                       passed to next container step\n",
    "    mlpipeline_ui_metadata_path : Path where metadata is stored\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import os\n",
    "    print(os.listdir())\n",
    "    import json\n",
    "    import pandas\n",
    "    import gcsfs\n",
    "    import pandas as pd\n",
    "    \n",
    "    file_list = gcs_path.split(',')\n",
    "    \n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'secrets.json'\n",
    "    fs = gcsfs.GCSFileSystem(project='leafy-ether-314809' , token='secrets.json')\n",
    "    \n",
    "    # Note : Here I have read only 1 file, we can create empty dataframe and \n",
    "    read all the csv files passed in it with concat on axis 0\n",
    "    with fs.open(file_list[0]) as f:\n",
    "        loan_data = pandas.read_csv(f)\n",
    "        \n",
    "    # keeping validation set aside\n",
    "    X = loan_data.iloc[:,loan_data.columns!='Loan_Status']\n",
    "    y = loan_data.iloc[:,loan_data.columns=='Loan_Status']\n",
    "    X, Xval, y, yval = train_test_split(X,y,test_size=0.15, random_state=45)\n",
    "    loan_data = pd.concat([X,y],axis=1).reset_index(drop=True)\n",
    "    Xval,yval = Xval.reset_index(drop=True),yval.reset_index(drop=True)\n",
    "    \n",
    "    # Send output for next container step\n",
    "    loan_data.to_csv(output_csv,index=False)\n",
    "    \n",
    "    # Log train data files\n",
    "    file_list = [i+'#'+fs.stat(i)['generation'] for i in file_list]\n",
    "    metadata = {\n",
    "    'outputs' : [\n",
    "    # Markdown that is hardcoded inline\n",
    "    {\n",
    "      'storage': 'inline',\n",
    "      'source': '# Training files used\\n'+ ','.join(file_list),\n",
    "      'type': 'markdown',\n",
    "    }]\n",
    "      }\n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    #Save splitted data to validation forlder if required\n",
    "\n",
    "\n",
    "# Step 2 of Kubeflow pipeline \n",
    "@dsl.python_component(\n",
    "    name='preprocess',\n",
    "    description='',\n",
    "    base_image=CONTAINER_NAME # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "def preprocess(text_path: InputPath(),output_csv: OutputPath(str),imputer_path: OutputPath(str)):\n",
    "    \"\"\"\n",
    "    Data preprocessing step\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    text_path (str) : Path of input training data\n",
    "    output_csv (str) : (internally assigned by kfp) Path where output csv will be stored and \n",
    "                       passed to next container step\n",
    "    imputer_path (str) : (internally assigned by kfp) Path where imputer instance will be stored and \n",
    "                       passed to training container step\n",
    "  \n",
    "    \"\"\"\n",
    "    import os\n",
    "    print(os.listdir())\n",
    "    import sys\n",
    "    sys.path.append('.')\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from utils.impute_class import impute\n",
    "    \n",
    "    global imputer_cls\n",
    "    loan_data = pd.read_csv(text_path)\n",
    "    imputer = impute()\n",
    "    loan_data = imputer.fit(loan_data)\n",
    "    loan_data.to_csv(output_csv,index=False)\n",
    "    \n",
    "    imputer_cls = imputer\n",
    "    import joblib\n",
    "    import dill\n",
    "    with open(imputer_path, \"wb\") as dill_file:\n",
    "        dill.dump([imputer_cls],dill_file)\n",
    "    \n",
    "    \n",
    "# Step 3 of Kubeflow pipeline \n",
    "@dsl.python_component(\n",
    "    name='FE',\n",
    "    description='adds two numbers',\n",
    "    base_image=CONTAINER_NAME  # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "def FE(text_path: InputPath(),output_csv: OutputPath(str),FE_path: OutputPath(str)):\n",
    "    \"\"\"\n",
    "    Feature engineering step\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    text_path (str) : Path of input training data\n",
    "    output_csv (str) : (internally assigned by kfp) Path where output csv will be stored and \n",
    "                       passed to next container step\n",
    "    FE_path (str) : (internally assigned by kfp) Path where Feature engineering instance will be \n",
    "                        stored and passed to training container step\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import sys\n",
    "    import joblib\n",
    "    global FE_cls\n",
    "    import dill\n",
    "    sys.path.append('.')\n",
    "    from utils.FE import Feature_engineering\n",
    "    \n",
    "    ###################\n",
    "    \n",
    "    loan_data = pd.read_csv(text_path)\n",
    "    FE_pipeline = Feature_engineering()\n",
    "    loan_data = FE_pipeline.fit(loan_data)\n",
    "    loan_data.to_csv(output_csv,index=False)\n",
    "    \n",
    "    FE_cls = FE_pipeline\n",
    "    with open(FE_path, \"wb\") as dill_file:\n",
    "        dill.dump([FE_cls],dill_file)\n",
    "        \n",
    "# Step 4 of Kubeflow pipeline \n",
    "@dsl.python_component(\n",
    "    name='train',\n",
    "    description='',\n",
    "    base_image=CONTAINER_NAME  # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "def train(text_path: InputPath(),imputer_path: InputPath(), FE_path :  InputPath(), mlpipeline_metrics_path: OutputPath('Metrics')):\n",
    "    \"\"\"\n",
    "    Model training step\n",
    "  \n",
    "    Parameters\n",
    "    ----------\n",
    "    text_path (str) : Path of input training data\n",
    "    imputer_path (str) : (internally assigned by kfp) Path where imputer instance pkl is stored\n",
    "    FE_path (str) : (internally assigned by kfp) Path where Feature engineering pkl is stored\n",
    "    mlpipeline_metrics_path : Path where metrics are stored\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import f1_score\n",
    "    import dill\n",
    "    import sys\n",
    "    import gcsfs\n",
    "    import os\n",
    "    sys.path.append('.')\n",
    "    \n",
    "    # Load train data\n",
    "    loan_data = pd.read_csv(text_path)\n",
    "    \n",
    "    loan_data['Loan_Status']=loan_data['Loan_Status'].astype('int')\n",
    "    loan_data = loan_data[loan_data>=0].dropna()\n",
    "\n",
    "    X = loan_data.iloc[:,loan_data.columns!='Loan_Status']\n",
    "    y = loan_data.iloc[:,loan_data.columns=='Loan_Status']\n",
    "\n",
    "    # Split data into train and test set\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=0.10, random_state=45) # creating train test split\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg_model = log_reg.fit(Xtrain,ytrain) # classifier function will train the MLmodel\n",
    "    ypred = log_reg_model.predict(Xtest) # Performing perdiction on test test\n",
    "    f1 = f1_score(y_true=ytest,y_pred=log_reg_model.predict(Xtest)) # Getting f1 score on test dataset\n",
    "    \n",
    "    # Log metrics\n",
    "    import json\n",
    "    accuracy = 0.9\n",
    "    metrics = {\n",
    "    'metrics': [{\n",
    "      'name': 'accuracy-score', # The name of the metric. Visualized as the column name in the runs table.\n",
    "      'numberValue':  accuracy, # The value of the metric. Must be a numeric value.\n",
    "      'format': \"PERCENTAGE\",   # The optional format of the metric. Supported values are \"RAW\" (displayed in raw format) and \"PERCENTAGE\" (displayed in percentage format).\n",
    "    }]\n",
    "    }\n",
    "    with open(mlpipeline_metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "    \n",
    "    # Load imputer and feature engineering pipeline which we saved and passed from\n",
    "    # previous container steps\n",
    "    with open(imputer_path, \"rb\") as imputer_file, open(FE_path, 'rb') as FE_file:\n",
    "        imputer = dill.load(imputer_file)[0]\n",
    "        FE_pipeline = dill.load(FE_file)[0]\n",
    "        \n",
    "    # Save model file into current container\n",
    "    model_file = (r'loan_model.pkl')\n",
    "    with open(model_file, \"wb\") as dill_file:\n",
    "        dill.dump([imputer,FE_pipeline, log_reg_model],dill_file)\n",
    "    \n",
    "    # Connect to GCS\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'secrets.json'\n",
    "    fs = gcsfs.GCSFileSystem(project='leafy-ether-314809' , token='secrets.json',cache_timeout=0)\n",
    "    \n",
    "    Production_model_f1 = 0.8\n",
    "    # If no files present then save first model in folder 01(version)\n",
    "    if len(fs.ls('gs://loan_model_pipeline')) == 0 :\n",
    "        # Upload model to GCS\n",
    "        with open(\"loan_model.pkl\", \"rb\") as local_file:\n",
    "            with fs.open(\"gs://loan_model_pipeline/\" + \"1/loan_model.pkl\", \"wb\") as gcs_file:\n",
    "                gcs_file.write(local_file.read())\n",
    "\n",
    "    # Save model to new folder if better than production model            \n",
    "    elif f1 > Production_model_f1: # production model f1 score\n",
    "        gcs_files = [i.replace('loan_model_pipeline/','') for i in fs.ls('gs://loan_model_pipeline/')]\n",
    "        next_folder_num = str(int(gcs_files[-1]) + 1)\n",
    "        with open(\"loan_model.pkl\", \"rb\") as local_file:\n",
    "            with fs.open(\"gs://loan_model_pipeline/\" + next_folder_num + \"/loan_model.pkl\", \"wb\") as gcs_file:\n",
    "                gcs_file.write(local_file.read())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02797f7e",
   "metadata": {},
   "source": [
    "##### pipeline_publisher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a840fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline/pipeline_publisher.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline/pipeline_publisher.py\n",
    "#This file compiles all the pipeline steps and saves yaml file\n",
    "from pipeline_components import *\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import compiler\n",
    "from kfp import components\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "# Convert read_and_split function to a pipeline operation.\n",
    "read_split_op = components.func_to_container_op(\n",
    "    read_and_split,\n",
    "    base_image=CONTAINER_NAME,\n",
    "    packages_to_install=['pandas==1.1.4','gcsfs','scikit-learn'] # optional\n",
    ")\n",
    "\n",
    "# Convert preprocessing function to a pipeline operation.\n",
    "preprocess_op = components.func_to_container_op(\n",
    "    preprocess,\n",
    "    base_image=CONTAINER_NAME,\n",
    "    packages_to_install=['pandas==1.1.4','gcsfs','joblib','dill'] # optional\n",
    ")\n",
    "\n",
    "# Convert feature engineering function to a pipeline operation.\n",
    "FE_op = components.func_to_container_op(\n",
    "    FE,\n",
    "    base_image=CONTAINER_NAME,\n",
    "    packages_to_install=['pandas==1.1.4','gcsfs','scikit-learn','dill'] # optional\n",
    ")\n",
    "\n",
    "# Convert training function to a pipeline operation.\n",
    "train_op = components.func_to_container_op(\n",
    "    train,\n",
    "    base_image=CONTAINER_NAME,\n",
    "    packages_to_install=['pandas==1.1.4','gcsfs','scikit-learn','joblib','dill'] # optional\n",
    ")\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "@dsl.pipeline(\n",
    "   name='Calculation pipeline',\n",
    "   description='A toy pipeline that performs arithmetic calculations.'\n",
    ")\n",
    "def ds_pipeline(\n",
    "   gcs_path: str,\n",
    "                ):\n",
    "    #Passing pipeline parameter and a constant value as operation arguments\n",
    "    read_split = read_split_op(gcs_path) #Returns a dsl.ContainerOp class instance. \n",
    "    read_split.container.set_image_pull_policy('Always') # Because by default it uses cached image\n",
    "    \n",
    "    preprocess=preprocess_op(read_split.outputs['output_csv'])\n",
    "    preprocess.container.set_image_pull_policy('Always')\n",
    "    \n",
    "    FE = FE_op(preprocess.outputs['output_csv'])\n",
    "    FE.container.set_image_pull_policy('Always') \n",
    "    \n",
    "    train = train_op(FE.outputs['output_csv'],preprocess.outputs['imputer'],FE.outputs['FE'])\n",
    "    train.container.set_image_pull_policy('Always')\n",
    "\n",
    "    \n",
    "# Combine pipeline and save yaml file\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=ds_pipeline,\n",
    "    package_path='pipeline/ds_train.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece235fd",
   "metadata": {},
   "source": [
    "##### pipeline_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3c0b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline/pipeline_run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline/pipeline_run.py\n",
    "\n",
    "# This file runs a saved pipeline and also if new model is registered then it saves \n",
    "# its compiled pipeline file yaml with it\n",
    "\n",
    "# Check gcs storage\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import gcsfs\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to GCS and check current model version\n",
    "import gcsfs\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'secrets.json'\n",
    "fs = gcsfs.GCSFileSystem(project='leafy-ether-314809' , token='secrets.json',cache_timeout=0)\n",
    "initial_length = len(fs.ls('gs://loan_model_pipeline'))\n",
    "\n",
    "# Run kfp pipeline\n",
    "import kfp\n",
    "# Set up Kubeflow client using its url\n",
    "client = kfp.Client('https://2886795272-31380-shadow05.environments.katacoda.com/pipeline/')\n",
    "run = client.create_run_from_pipeline_package(\n",
    "        pipeline_file='pipeline/ds_train.yaml',\n",
    "        arguments = {'gcs_path': 'gs://bucket-306/data/train/dataloan.csv' },experiment_name='MLOps_prod'\n",
    "        )\n",
    "\n",
    "client.wait_for_run_completion(run.run_id, 3600)\n",
    "\n",
    "# If model was registered then save this pipeline to model's file\n",
    "new_length = len(fs.ls('gs://loan_model_pipeline'))\n",
    "if new_length > initial_length:\n",
    "    gcs_files = [i.replace('loan_model_pipeline/','') for i in fs.ls('gs://loan_model_pipeline/')]\n",
    "    folder_num = gcs_files[-1]\n",
    "    with open(\"pipeline/ds_train.yaml\", \"rb\") as local_file:\n",
    "        with fs.open(\"gs://loan_model_pipeline/\" + folder_num + \"/model_pipeline.yaml\", \"wb\") as gcs_file:\n",
    "            gcs_file.write(local_file.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab760f6",
   "metadata": {},
   "source": [
    "##### cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1594950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline/cloudbuild.yaml\n",
    "steps:\n",
    "  # this builds base image to use it for kubeflow pipelines\n",
    "  - name: 'gcr.io/cloud-builders/docker'\n",
    "    id: base_image_creation\n",
    "    entrypoint: /bin/sh\n",
    "    args:\n",
    "    - -c\n",
    "    - \"bash pipeline/pipeline_base_image_builder.sh\"\n",
    "\n",
    "  # this runs files required for training \n",
    "  - name: 'python'\n",
    "    env:\n",
    "    - 'NEW_CONTAINER=${_CONTAINER_NAME}'\n",
    "    id: train_model\n",
    "    entrypoint: /bin/sh\n",
    "    args:\n",
    "    - -c\n",
    "    - \"pip install -r requirements.txt && python pipeline/pipeline_publisher.py \\\n",
    "\t&& python pipeline/pipeline_run.py\"\n",
    "  # In above line we can add execute testing scripts as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1ebdc",
   "metadata": {},
   "source": [
    "##### Dockerfile"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39469750",
   "metadata": {},
   "source": [
    "This creates dockerfile for creating base image which is used for kubeflow pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ceaa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/Dockerfile\n",
    "FROM python:3.7-slim\n",
    "COPY ./requirements.txt ./secrets.json ./\n",
    "COPY ./utils utils\n",
    "RUN pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2e41c",
   "metadata": {},
   "source": [
    "##### pipeline_base_image_builder.sh"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3228b564",
   "metadata": {},
   "source": [
    "This file runs few commands required for docker build and push to repository\n",
    "Note: we can also use google cloud container registry as well, to store image, Here I have\n",
    "      used dockerhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/pipeline_base_image_builder.sh\n",
    "export CONTAINER_NAME=\"$(git log -1 --pretty=%h)\"\n",
    "docker login --username=$(docker_usrname) --password=$(docker_password)\n",
    "docker build -t vinodswnt306/new_public_mlops:$CONTAINER_NAME -f ./pipeline/Dockerfile .\n",
    "docker push vinodswnt306/new_public_mlops:$CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190b06f",
   "metadata": {},
   "source": [
    "#### 1.2 Details of “deployment” folder"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1b38ded",
   "metadata": {},
   "source": [
    "Note : Following code for section 1.2 is not yet implemented, it is just a sample code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21a799af",
   "metadata": {},
   "source": [
    "+---deployment\n",
    "|       score.py\n",
    "|       requirements.txt\n",
    "|       deployment.yaml\n",
    "|       service.yaml\n",
    "|       cloudbuild.yaml\n",
    "|       Dockerfile\n",
    "|       scoring_image_builder.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db6c55",
   "metadata": {},
   "source": [
    "##### score.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac20147e",
   "metadata": {},
   "source": [
    "In this file we will write flask app which loads model and predicts on input json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b276696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "import dill\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "target={0:'Accept', 1:'Reject'}\n",
    "\n",
    "a,b,c = dill.load('models/loan_model.pkl') \n",
    "\n",
    "def scorer(text):\n",
    "    encoded_text = a.transform([text])\n",
    "    score = c.predict(xgb.DMatrix(encoded_text))\n",
    "    return score\n",
    "\n",
    "@app.route('/score', methods=['POST'])\n",
    "def predict_fn():\n",
    "    data = request.get_json()['data']\n",
    "    predictions = scorer(data)\n",
    "    return jsonify({'predictions ': str(predictions), 'Category ': target.get(predictions)})\n",
    "\n",
    "@app.route('/')\n",
    "def hello():\n",
    "    return 'Welcome to Loan Prediction Application'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb31c2f",
   "metadata": {},
   "source": [
    "##### requirements.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f1a0881",
   "metadata": {},
   "source": [
    "In this file we will write dependencies that model prediction require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "flask\n",
    "scikit-learn==0.22\n",
    "dill\n",
    "gunicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65aed7",
   "metadata": {},
   "source": [
    "##### deployment.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acac140a",
   "metadata": {},
   "source": [
    "In this file we will write kubernetes config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa375aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: Loan_Status_app\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: Loan_Status\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: Loan_Status\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: Loan-app\n",
    "        image: gcr.io/gcp-repo-name/Loan_image\n",
    "        ports:\n",
    "        - containerPort: 8080\n",
    "        env:\n",
    "          - name: PORT\n",
    "            value: \"8080\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4a6af",
   "metadata": {},
   "source": [
    "#####  service.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e687f11d",
   "metadata": {},
   "source": [
    "In this file we will write on which service we want to run our flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: Loan_Status\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: Loan_Status\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd9237",
   "metadata": {},
   "source": [
    "##### cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "756c2c7c",
   "metadata": {},
   "source": [
    "In this file we will write cloud build code for Continuous deployment pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fdd8bcf2",
   "metadata": {},
   "source": [
    "Steps :\n",
    "1) Test scoring script\n",
    "2) Build container with scoring script\n",
    "3) Deploy container to GKE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30199ca",
   "metadata": {},
   "source": [
    "##### Dockerfile"
   ]
  },
  {
   "cell_type": "raw",
   "id": "997c4c6d",
   "metadata": {},
   "source": [
    "In this file we will write script to build container which runs score.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8480c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightweight python\n",
    "FROM python:3.7-slim\n",
    "\n",
    "# Copy local code to the container image.\n",
    "ENV APP_HOME /app\n",
    "WORKDIR $APP_HOME\n",
    "COPY ./deployment/* ./*\n",
    "COPY ./utils ./utils\n",
    "\n",
    "RUN ls -la $APP_HOME/\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "ENV PORT 5000\n",
    "\n",
    "# Run the flask service on container startup\n",
    "#CMD exec gunicorn --bind :$PORT --workers 1 --threads 8 ComplaintsServer\n",
    "CMD [ \"python\", \"score.py\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa5828",
   "metadata": {},
   "source": [
    "##### scoring_image_builder.sh"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7fe3624",
   "metadata": {},
   "source": [
    "%%writefile deployment/scoring_image_builder.sh\n",
    "docker login --username=$(docker_usrname) --password=$(docker_password)\n",
    "docker build -t gcr.io/gcp-repo-name/Loan_image -f ./deployment/Dockerfile .\n",
    "docker push gcr.io/gcp-repo-name/Loan_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ac1fd",
   "metadata": {},
   "source": [
    "#### 1.3 Details of “tests” folder"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bf40804",
   "metadata": {},
   "source": [
    "+---tests\n",
    "|       test_1_FE.py\n",
    "|       test_2_impute_class.py\n",
    "|       test_3_score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f231c130",
   "metadata": {},
   "source": [
    "##### test_1_FE.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87339888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import sys, os.path\n",
    "\n",
    "py_scrpt_dir = (os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "sys.path.append(py_scrpt_dir)\n",
    "from utils.FE import *\n",
    "\n",
    "FE_test_inputs = pd.read_csv(r'.\\data\\FE_test_inputs.csv')\n",
    "\n",
    "def test_func1():\n",
    "    global FE_test_inputs\n",
    "    FE_pipeline = Feature_engineering()\n",
    "    FE_test_output = FE_pipeline.fit(FE_test_inputs)\n",
    "    \n",
    "    number_of_non_numeric_columns = FE_test_output.select_dtypes(exclude=['int64','float64']).shape[1]\n",
    "    assert number_of_non_numeric_columns == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c6929",
   "metadata": {},
   "source": [
    "#####  test_2_impute_class.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689a3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import sys, os.path\n",
    "\n",
    "py_scrpt_dir = (os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "sys.path.append(py_scrpt_dir)\n",
    "from utils.impute_class import *\n",
    "\n",
    "loan_data = pd.read_csv(r'.\\data\\dataloan.csv')\n",
    "\n",
    "def test_func1(): \n",
    "    global loan_data\n",
    "    imputer = impute()\n",
    "    loan_data = imputer.fit(loan_data)\n",
    "    number_of_columns_with_NA = loan_data.isna().sum()[loan_data.isna().sum() > 0].shape[0]\n",
    "\n",
    "    assert number_of_columns_with_NA == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f53f7",
   "metadata": {},
   "source": [
    "#####  test_3_score.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ffea367",
   "metadata": {},
   "source": [
    "Here we write script to test functions in score.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045874d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47724259",
   "metadata": {},
   "source": [
    "#### 1.4 Details of “utils” folder "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d9ab191",
   "metadata": {},
   "source": [
    "+---utils\n",
    "|       FE.py\n",
    "|       impute_class.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194da6b3",
   "metadata": {},
   "source": [
    "##### FE.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Feature_engineering:\n",
    "    def __init__(self):\n",
    "        self.z=0\n",
    "        \n",
    "    def fit(self,loan_data):\n",
    "        \n",
    "        loan_data.drop(['Loan_ID'],axis=1,inplace=True)\n",
    "        \n",
    "        catgeorical_features = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area']\n",
    "        for feature in catgeorical_features:\n",
    "            loan_data[feature] = loan_data[feature].astype('category')\n",
    "        \n",
    "        X = loan_data.iloc[:,loan_data.columns!='Loan_Status']\n",
    "        y = loan_data.iloc[:,loan_data.columns=='Loan_Status']\n",
    "        \n",
    "        self.ohe = OneHotEncoder().fit(X.select_dtypes('category'))\n",
    "        catg_cols_transform = self.ohe.transform(X.select_dtypes('category')).toarray()\n",
    "        self.catg_feat_names = X.select_dtypes('category').columns\n",
    "        dfOneHot = pd.DataFrame(catg_cols_transform, columns = self.ohe.get_feature_names(self.catg_feat_names))\n",
    "        loan_data_OHE = pd.concat([X, dfOneHot], axis=1).drop(self.catg_feat_names,axis=1)\n",
    "        \n",
    "        loan_data = pd.concat([loan_data_OHE,y],axis=1)\n",
    "        \n",
    "        return loan_data\n",
    "        \n",
    "        \n",
    "    def transform(self,X):\n",
    "        X.drop(['Loan_ID'],axis=1,inplace=True)\n",
    "        catgeorical_features = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area']\n",
    "        for feature in catgeorical_features:\n",
    "            X[feature] = X[feature].astype('category')\n",
    "            \n",
    "        catg_cols_transform = self.ohe.transform(X.select_dtypes('category')).toarray()\n",
    "        dfOneHot = pd.DataFrame(catg_cols_transform, columns = self.ohe.get_feature_names(self.catg_feat_names))\n",
    "        loan_data_OHE = pd.concat([X, dfOneHot], axis=1).drop(self.catg_feat_names,axis=1)\n",
    "        \n",
    "        return loan_data_OHE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024228b",
   "metadata": {},
   "source": [
    "##### impute_class.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f861296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class impute:\n",
    "    def __init__(self):\n",
    "        self.imputer_dict_for_prod = {}\n",
    "        \n",
    "    def fit(self,loan_data):\n",
    "        cat_df = loan_data.drop(['Loan_ID'],axis=1).iloc[:,0:-1].select_dtypes(exclude=['int64','float64'])\n",
    "        num_df = loan_data.drop(['Loan_ID'],axis=1).iloc[:,0:-1].select_dtypes(include=['int64','float64'])\n",
    "        mode_impute_dict = cat_df.mode().iloc[0]\n",
    "        mean_impute_dict = dict(num_df.mean())\n",
    "        \n",
    "        self.imputer_dict_for_prod = {**mode_impute_dict, **mean_impute_dict}\n",
    "        \n",
    "        cat_df.fillna(cat_df.mode().iloc[0],inplace=True)\n",
    "        num_df.fillna(num_df.mean(),inplace=True)\n",
    "        \n",
    "        loan_data = pd.concat([cat_df,num_df,loan_data[['Loan_ID','Loan_Status']]],axis=1)\n",
    "        \n",
    "        return loan_data\n",
    "        \n",
    "        \n",
    "    def transform(self,df):\n",
    "        for i,j in self.imputer_dict_for_prod.items():\n",
    "            df[i].fillna(j, inplace=True) \n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d4d17",
   "metadata": {},
   "source": [
    "# Step 2 : Setup Cloud Build trigger for CI pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece59644",
   "metadata": {},
   "source": [
    "#### 2.1 Details of “train_pipeline/cloudbuild.yaml” file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps:\n",
    "  # Docker Build base image for pipeline\n",
    "  - name: 'gcr.io/cloud-builders/docker'\n",
    "    id: image\n",
    "    entrypoint: /bin/sh\n",
    "    args:\n",
    "    - -c\n",
    "    - \"bash docker/pipeline_base_image_builder.sh\"\n",
    "    \n",
    "  # Compine and run pipeline  \n",
    "  - name: 'python'\n",
    "    id: run\n",
    "    entrypoint: /bin/sh\n",
    "    args:\n",
    "    - -c\n",
    "    - \"pip install -r requirements.txt && python pipeline/pipeline_publisher.py \\\n",
    "\t&& python pipeline/pipeline_run.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f6ba2",
   "metadata": {},
   "source": [
    "#### 2.2 Set up CI pipeline on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Steps :\n",
    "1) Go to cloud build page on GCP console\n",
    "2) Click on create trigger\n",
    "3) Give name for trigger and select and link your desired Git/CSR/BitBucket repository for trigger\n",
    "4) Give path of cloudbuild.yaml \n",
    "5) Save trigger and exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6d092",
   "metadata": {},
   "source": [
    "# Step 3 : Setup Cloud Build trigger for CD pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e7a31",
   "metadata": {},
   "source": [
    "#### 3.1 Details of “deployment /cloudbuild.yaml” file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001548fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e15b2343",
   "metadata": {},
   "source": [
    "#### 3.2 Set up CD pipeline on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7eba3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6be409f3",
   "metadata": {},
   "source": [
    "# Step 4 : Setup cloud function triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc803d",
   "metadata": {},
   "source": [
    "#### 4.1 CD pipeline trigger on model update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8091d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "It will be triggered based on changes in bucket where model is stored\n",
    "Any new version update will trigger this function and it will run CD pipeline\n",
    "\n",
    "Steps :\n",
    "1) Clone repository\n",
    "2) Run deployment/cloudbuild.yaml using sdk "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42188e4",
   "metadata": {},
   "source": [
    "#### 4.2 Setting up cloud function trigger for retraining (run yaml from model folder on data change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "It will be triggered based on changes in bucket where data is stored\n",
    "Any new version update will trigger this function and it will run CI pipeline\n",
    "\n",
    "Steps :\n",
    "1) run pipeline.yaml from latest model folder, using kubeflow client.run "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
